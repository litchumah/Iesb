{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício 3 - Análise de sentimento no Twitter\n",
    "\n",
    "Nesse exercício vamos analisar uma base de dados do Twitter. Vamos também aplicar um classificador deep learning em nossa base e comparar com um classificador mais básico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense\n",
    "from keras.layers import SimpleRNN, LSTM, GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos a base de dados Sentiment140, com sentimentos de tweets organizada pela Universidade de Stanford. Informações sobre a base podem ser encontradas no link (http://help.sentiment140.com/for-students). O formato da base no link é diferente do formato que usaremos nesse exercício, mas os dados são os mesmos. Assim, utilize a base de dados do Blackboard ou do Google Drive. É recomendado que o exercício seja executado no Google Colab, utilizando GPU ou TPU para o kernel, já que usaremos o Keras. \n",
    "\n",
    "É necessário descompactar o arquivo. Caso esteja executando no Google Colab, execute o procedimento de montagem do drive no Colab e descompacte o arquivo.\n",
    "\n",
    "Vamos carregar a base de treinamento. Lembre-se de informar o caminho do arquivo no comando abaixo. Aqui consideramos que o arquivo de treinamento está disponível na pasta data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['sentiment','id','date','query_string','user','text']\n",
    "training_df = pd.read_csv(\"./data/training.1600000.processed.noemoticon.csv\",header=None, names=cols, encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos definir alguns parâmetros importantes. Primeiro definimos a quantidade de itens que vamos utilizar da base. A base possui 160000 registros, o que é pesado para analisarmos com deep learning. Definimos também a quantidade máxima de palavras que consideraremos na tokenização. O processo de tokenização que usaremos cria um dicionário de palavras e retorna um vetor com o índice de calada palavra no dicionário. Como estamos lidando com redes sociais, temos diversas palavras únicas, o que criaria um dicionário muito grande, por isso limitamos esse dicionário. Por fim, definimos o tamanho máximo de cada sentença, a fim de limitar o tamanho de cada vetor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_limit = 200000\n",
    "max_words = 100000\n",
    "max_len = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos tratar nossos dados, removendo html e caracteres estranhos. Para isso criamos uma função que trata cada tweet em separado. Depois aplicamos essa função em cada tweet, já limitando ao nosso limite definido acima. Vamos aproveitar e embaralhar os dados, já que as classes estão ordenadas no dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df.drop(['id','date','query_string','user'],axis=1,inplace=True)\n",
    "train_df = training_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "def tweet_cleaner(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    stripped = re.sub(combined_pat, '', souped)\n",
    "    try:\n",
    "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = stripped\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "    lower_case = letters_only.lower()\n",
    "    return lower_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_training_texts = []\n",
    "for i in range(0, data_limit):\n",
    "    if( (i+1)%100000 == 0 ):\n",
    "        print(\"Tweets %d of %d has been processed\" % ( i+1, data_limit ))\n",
    "    clean_training_texts.append(tweet_cleaner(train_df['text'][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos tokenizar os tweets utilizando o Tokenizador do Keras. Esse tokenizador tem a vantagem de retornar sequências de números ao invés da sentença tokenizada. Essa sequência possuí índices das palavras em um dicionário com todas as palavras na lista. Isso é importante para que nossa rede possa treinar embeddings para as palavras. Aqui limitamos a quantidade de palavras no dicionário pelo parâmetro max_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_words,lower=True, split=\" \")\n",
    "tokenizer.fit_on_texts(clean_training_texts)\n",
    "tokens = tokenizer.texts_to_sequences(clean_training_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos limitar nossas sequências para um número definido de valores, adicionando zeros caso a sequência seja menor ou cortando a sequência caso seja maior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sequence.pad_sequences(tokens, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamor gerar nosso conjunto de labels (y). Para isso, retiramos do dataframe inicial os registros da coluna sentiment limitando ao parâmetro data_limit. Fazemos um tratamento no conjunto para que esse seja binário. No dataset original, o rótulo 0 indica negativo e o rótulo 4 indica positivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_df.sentiment[:data_limit]\n",
    "y = [1 if v == 4 else 0 for v in y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos criar nossa rede de classificação. Para isso, crie uma rede recorrente que classifique nossos tweets. A rede deve obrigatoriamente ter uma camada de embedding, uma camada recorrente (SimpleRNN, GRU ou LSTM) e a camada de saída da classificação (camada densa com 1 unidade, de preferência com ativação sigmoide). Você está livre para criar quantas camadas desejar, lembrando que quanto mais camadas, mais demorado é o treinamento da rede."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "#Crie sua rede aqui\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos treinar nossa rede. Aqui está definido alguns parâmetros para treinamento, sinta-se a vontade para alterar esses parâmetros conforme desejar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X, y, epochs=10, batch_size=128, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifique a perda e a acurácia de validação durante o treinamento. Verifique se seu modelo está performando bem com os parâmetros e arquitetura de rede escolhida. Caso deseje, utilize o history para mostrar gráficos com os dados do treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Espaço para gráficos caso deseje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos comparar com um classificador clássico, a Regressão Logística. Primeiramente vamos vetorizar o texto usando o TF-IDF. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#complete as linhas com o TfidfVectorizer\n",
    "tfidf = \n",
    "X = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separe em conjunto de teste e validação, usando o train_test_split. Aqui vamos manter a mesma taxa de validação (test_size) usada ao treinar a rede recorrente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#complete a linha abaixo\n",
    "X_train, X_val, y_train, y_val = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crie a regressão logistica e treine o modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#complete as linhas abaixo\n",
    "lr = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora ver a acurácia do modelo. A acurácia é muito diferente do modelo deep learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Accuracy : %s\" % (accuracy_score(y_val, lr.predict(X_val))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
