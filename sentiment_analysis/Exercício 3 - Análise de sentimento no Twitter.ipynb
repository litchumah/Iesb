{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício 3 - Análise de sentimento no Twitter\n",
    "\n",
    "Nesse exercício vamos analisar uma base de dados do Twitter. Vamos também aplicar um classificador deep learning em nossa base e comparar com um classificador mais básico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.preprocessing.text import Tokenizer\n",
    "# from keras.preprocessing import sequence\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Embedding\n",
    "# from keras.layers import Dense\n",
    "# from keras.layers import SimpleRNN, LSTM, GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import SimpleRNN, LSTM, GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos a base de dados Sentiment140, com sentimentos de tweets organizada pela Universidade de Stanford. Informações sobre a base podem ser encontradas no link (http://help.sentiment140.com/for-students). O formato da base no link é diferente do formato que usaremos nesse exercício, mas os dados são os mesmos. Assim, utilize a base de dados do Blackboard ou do Google Drive. É recomendado que o exercício seja executado no Google Colab, utilizando GPU ou TPU para o kernel, já que usaremos o Keras. \n",
    "\n",
    "É necessário descompactar o arquivo. Caso esteja executando no Google Colab, execute o procedimento de montagem do drive no Colab e descompacte o arquivo.\n",
    "\n",
    "Vamos carregar a base de treinamento. Lembre-se de informar o caminho do arquivo no comando abaixo. Aqui consideramos que o arquivo de treinamento está disponível na pasta data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['sentiment','id','date','query_string','user','text']\n",
    "training_df = pd.read_csv(\"./twitter_sentiment/training.1600000.processed.noemoticon.csv\",header=None, names=cols, encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos definir alguns parâmetros importantes. Primeiro definimos a quantidade de itens que vamos utilizar da base. A base possui 160000 registros, o que é pesado para analisarmos com deep learning. Definimos também a quantidade máxima de palavras que consideraremos na tokenização. O processo de tokenização que usaremos cria um dicionário de palavras e retorna um vetor com o índice de calada palavra no dicionário. Como estamos lidando com redes sociais, temos diversas palavras únicas, o que criaria um dicionário muito grande, por isso limitamos esse dicionário. Por fim, definimos o tamanho máximo de cada sentença, a fim de limitar o tamanho de cada vetor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_limit = 200000\n",
    "max_words = 100000\n",
    "max_len = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos tratar nossos dados, removendo html e caracteres estranhos. Para isso criamos uma função que trata cada tweet em separado. Depois aplicamos essa função em cada tweet, já limitando ao nosso limite definido acima. Vamos aproveitar e embaralhar os dados, já que as classes estão ordenadas no dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df.drop(['id','date','query_string','user'],axis=1,inplace=True)\n",
    "train_df = training_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "def tweet_cleaner(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    stripped = re.sub(combined_pat, '', souped)\n",
    "    try:\n",
    "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = stripped\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "    lower_case = letters_only.lower()\n",
    "    return lower_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets 100000 of 200000 has been processed\n",
      "Tweets 200000 of 200000 has been processed\n"
     ]
    }
   ],
   "source": [
    "clean_training_texts = []\n",
    "for i in range(0, data_limit):\n",
    "    if( (i+1)%100000 == 0 ):\n",
    "        print(\"Tweets %d of %d has been processed\" % ( i+1, data_limit ))\n",
    "    clean_training_texts.append(tweet_cleaner(train_df['text'][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos tokenizar os tweets utilizando o Tokenizador do Keras. Esse tokenizador tem a vantagem de retornar sequências de números ao invés da sentença tokenizada. Essa sequência possuí índices das palavras em um dicionário com todas as palavras na lista. Isso é importante para que nossa rede possa treinar embeddings para as palavras. Aqui limitamos a quantidade de palavras no dicionário pelo parâmetro max_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_words,lower=True, split=\" \")\n",
    "tokenizer.fit_on_texts(clean_training_texts)\n",
    "tokens = tokenizer.texts_to_sequences(clean_training_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos limitar nossas sequências para um número definido de valores, adicionando zeros caso a sequência seja menor ou cortando a sequência caso seja maior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sequence.pad_sequences(tokens, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamor gerar nosso conjunto de labels (y). Para isso, retiramos do dataframe inicial os registros da coluna sentiment limitando ao parâmetro data_limit. Fazemos um tratamento no conjunto para que esse seja binário. No dataset original, o rótulo 0 indica negativo e o rótulo 4 indica positivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_df.sentiment[:data_limit]\n",
    "y = [1 if v == 4 else 0 for v in y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos criar nossa rede de classificação. Para isso, crie uma rede recorrente que classifique nossos tweets. A rede deve obrigatoriamente ter uma camada de embedding, uma camada recorrente (SimpleRNN, GRU ou LSTM) e a camada de saída da classificação (camada densa com 1 unidade, de preferência com ativação sigmoide). Você está livre para criar quantas camadas desejar, lembrando que quanto mais camadas, mais demorado é o treinamento da rede."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_words,32))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#Crie sua rede aqui\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos treinar nossa rede. Aqui está definido alguns parâmetros para treinamento, sinta-se a vontade para alterar esses parâmetros conforme desejar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 150000 samples, validate on 50000 samples\n",
      "Epoch 1/10\n",
      "150000/150000 [==============================] - 83s 554us/sample - loss: 0.6040 - acc: 0.6718 - val_loss: 0.5886 - val_acc: 0.6874\n",
      "Epoch 2/10\n",
      "150000/150000 [==============================] - 84s 557us/sample - loss: 0.5845 - acc: 0.6873 - val_loss: 0.5779 - val_acc: 0.6955\n",
      "Epoch 3/10\n",
      "150000/150000 [==============================] - 83s 555us/sample - loss: 0.5754 - acc: 0.6956 - val_loss: 0.5703 - val_acc: 0.7007\n",
      "Epoch 4/10\n",
      "150000/150000 [==============================] - 83s 556us/sample - loss: 0.5667 - acc: 0.7019 - val_loss: 0.5697 - val_acc: 0.7038\n",
      "Epoch 5/10\n",
      "150000/150000 [==============================] - 82s 548us/sample - loss: 0.5585 - acc: 0.7079 - val_loss: 0.5595 - val_acc: 0.7106\n",
      "Epoch 6/10\n",
      "150000/150000 [==============================] - 81s 540us/sample - loss: 0.5530 - acc: 0.7126 - val_loss: 0.5573 - val_acc: 0.7103\n",
      "Epoch 7/10\n",
      "150000/150000 [==============================] - 80s 532us/sample - loss: 0.5482 - acc: 0.7163 - val_loss: 0.5554 - val_acc: 0.7144\n",
      "Epoch 8/10\n",
      "150000/150000 [==============================] - 80s 531us/sample - loss: 0.5435 - acc: 0.7192 - val_loss: 0.5507 - val_acc: 0.7149\n",
      "Epoch 9/10\n",
      "150000/150000 [==============================] - 77s 511us/sample - loss: 0.5397 - acc: 0.7216 - val_loss: 0.5509 - val_acc: 0.7157\n",
      "Epoch 10/10\n",
      "150000/150000 [==============================] - 78s 517us/sample - loss: 0.5361 - acc: 0.7240 - val_loss: 0.5521 - val_acc: 0.7156\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(np.array(X, dtype=np.uint8), np.array(y, dtype=np.uint8), epochs=10, batch_size=128, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifique a perda e a acurácia de validação durante o treinamento. Verifique se seu modelo está performando bem com os parâmetros e arquitetura de rede escolhida. Caso deseje, utilize o history para mostrar gráficos com os dados do treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Espaço para gráficos caso deseje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos comparar com um classificador clássico, a Regressão Logística. Primeiramente vamos vetorizar o texto usando o TF-IDF. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#complete as linhas com o TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "X2 = tfidf.fit_transform(clean_training_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separe em conjunto de teste e validação, usando o train_test_split. Aqui vamos manter a mesma taxa de validação (test_size) usada ao treinar a rede recorrente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#complete a linha abaixo\n",
    "X_train, X_val, y_train, y_val = train_test_split(X2,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crie a regressão logistica e treine o modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefanini/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#complete as linhas abaixo\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora ver a acurácia do modelo. A acurácia é muito diferente do modelo deep learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.78876\n"
     ]
    }
   ],
   "source": [
    "print (\"Accuracy : %s\" % (accuracy_score(y_val, lr.predict(X_val))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
